{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "179d30fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, json, random, re, itertools\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    set_seed,\n",
    ")\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac130c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA: True | Device: cuda\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "CLEAN_DIR    = PROJECT_ROOT / \"data\" / \"clean\"\n",
    "\n",
    "TRAIN_JSONL  = CLEAN_DIR / \"train.jsonl\"\n",
    "VAL_JSONL    = CLEAN_DIR / \"val.jsonl\"\n",
    "TEST_JSONL   = CLEAN_DIR / \"test.jsonl\"   \n",
    "\n",
    "OUTPUT_DIR   = PROJECT_ROOT / \"saved_models\" / \"distilgpt2-npc\"\n",
    "\n",
    "MODEL_NAME        = \"distilgpt2\"\n",
    "EPOCHS            = 3\n",
    "BLOCK_SIZE        = 384      \n",
    "PER_DEVICE_BSZ    = 2\n",
    "GRAD_ACCUM        = 2\n",
    "LEARNING_RATE     = 2e-5\n",
    "WEIGHT_DECAY      = 0.01\n",
    "WARMUP_STEPS      = 200\n",
    "LOG_STEPS         = 50\n",
    "EVAL_STEPS        = 200\n",
    "SAVE_STEPS        = 200\n",
    "SEED              = 42\n",
    "\n",
    "\n",
    "MAX_NEW_TOKENS    = 64\n",
    "GENERATION_TOP_P  = 0.9\n",
    "GENERATION_TOP_K  = 50\n",
    "GENERATION_TEMP   = 0.8\n",
    "NO_REPEAT_NGRAM   = 3\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"CUDA:\", torch.cuda.is_available(), \"| Device:\", DEVICE)\n",
    "set_seed(SEED)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25b995b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added special tokens: 4 | Vocab size: 50261\n"
     ]
    }
   ],
   "source": [
    "SPECIAL_TOKENS = {\n",
    "    \"additional_special_tokens\": [\"<CONTEXT>\", \"<PLAYER>\", \"<NPC>\", \"<END>\"]\n",
    "}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "added = tokenizer.add_special_tokens(SPECIAL_TOKENS)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "if added > 0:\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(DEVICE)\n",
    "\n",
    "print(f\"Added special tokens: {added} | Vocab size: {len(tokenizer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2227d731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL counts -> train=4609 | val=255 | test=255\n"
     ]
    }
   ],
   "source": [
    "def load_jsonl(path: Path):\n",
    "    items = []\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            items.append(json.loads(line))\n",
    "    return items\n",
    "\n",
    "train_items = load_jsonl(TRAIN_JSONL)\n",
    "val_items   = load_jsonl(VAL_JSONL)\n",
    "test_items  = load_jsonl(TEST_JSONL) if TEST_JSONL.exists() else []\n",
    "\n",
    "print(f\"JSONL counts -> train={len(train_items)} | val={len(val_items)} | test={len(test_items)}\")\n",
    "assert len(train_items) > 0 and len(val_items) > 0, \"Train/Val JSONL must not be empty.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5decea20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'target', 'reference', 'npc_name'],\n",
       "        num_rows: 4609\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['prompt', 'target', 'reference', 'npc_name'],\n",
       "        num_rows: 255\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'target', 'reference', 'npc_name'],\n",
       "        num_rows: 255\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def to_hf_dataset(items):\n",
    "    rows = []\n",
    "    for it in items:\n",
    "        rows.append({\n",
    "            \"prompt\": it[\"prompt\"],\n",
    "            \"target\": it[\"target\"],             \n",
    "            \"reference\": it.get(\"reference\", \"\"),\n",
    "            \"npc_name\": it.get(\"npc_name\", \"\"),\n",
    "        })\n",
    "    return Dataset.from_list(rows)\n",
    "\n",
    "hf_train = to_hf_dataset(train_items)\n",
    "hf_val   = to_hf_dataset(val_items)\n",
    "hf_test  = to_hf_dataset(test_items) if test_items else None\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": hf_train,\n",
    "    \"validation\": hf_val,\n",
    "    **({\"test\": hf_test} if hf_test is not None else {}),\n",
    "})\n",
    "ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0effd44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8f8a6cbd1b34e3d9cbe2e38ad5ad61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4609 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94de0734b85a41498d7d8b5d38a1aa8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "001acfb4c38e41d5bef0129d7c617a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/255 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 4609\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 255\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 255\n",
      "    })\n",
      "})\n",
      "Example lengths: 68 68\n"
     ]
    }
   ],
   "source": [
    "def build_inputs(examples):\n",
    "    input_ids_list, attention_list, labels_list = [], [], []\n",
    "    prompts = examples[\"prompt\"]\n",
    "    targets = examples[\"target\"]\n",
    "\n",
    "    for p, t in zip(prompts, targets):\n",
    "        p_enc = tokenizer(p, add_special_tokens=False)\n",
    "        t_enc = tokenizer(t, add_special_tokens=False)\n",
    "\n",
    "        p_ids, p_att = p_enc[\"input_ids\"], p_enc[\"attention_mask\"]\n",
    "        t_ids, t_att = t_enc[\"input_ids\"], t_enc[\"attention_mask\"]\n",
    "\n",
    "        if len(p_ids) + len(t_ids) <= BLOCK_SIZE:\n",
    "            kept_p_ids, kept_p_att = p_ids, p_att\n",
    "            kept_t_ids, kept_t_att = t_ids, t_att\n",
    "        else:\n",
    "            min_target = 8\n",
    "            max_prompt_len = max(0, BLOCK_SIZE - min_target)\n",
    "            if len(p_ids) > max_prompt_len:\n",
    "                kept_p_ids = p_ids[-max_prompt_len:]\n",
    "                kept_p_att = p_att[-max_prompt_len:]\n",
    "                kept_t_ids = t_ids[:min_target]\n",
    "                kept_t_att = t_att[:min_target]\n",
    "            else:\n",
    "                room_for_target = BLOCK_SIZE - len(p_ids)\n",
    "                kept_p_ids, kept_p_att = p_ids, p_att\n",
    "                kept_t_ids = t_ids[:room_for_target]\n",
    "                kept_t_att = t_att[:room_for_target]\n",
    "\n",
    "        ids = kept_p_ids + kept_t_ids\n",
    "        att = kept_p_att + kept_t_att\n",
    "        labels = ([-100] * len(kept_p_ids)) + kept_t_ids\n",
    "\n",
    "        input_ids_list.append(ids)\n",
    "        attention_list.append(att)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "    return {\"input_ids\": input_ids_list, \"attention_mask\": attention_list, \"labels\": labels_list}\n",
    "\n",
    "tokenized = ds.map(build_inputs, batched=True, remove_columns=ds[\"train\"].column_names)\n",
    "\n",
    "print(tokenized)\n",
    "print(\"Example lengths:\",\n",
    "      len(tokenized[\"train\"][0][\"input_ids\"]),\n",
    "      len(tokenized[\"train\"][0][\"labels\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a067d02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids (4, 72)\n",
      "attention_mask (4, 72)\n",
      "labels (4, 72)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class DataCollatorForCausalSFT:\n",
    "    tokenizer: AutoTokenizer\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    label_pad_token_id: int = -100\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "        batch_inputs = {\n",
    "            \"input_ids\": [f[\"input_ids\"] for f in features],\n",
    "            \"attention_mask\": [f[\"attention_mask\"] for f in features],\n",
    "        }\n",
    "        batch = self.tokenizer.pad(\n",
    "            batch_inputs,\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        max_len = batch[\"input_ids\"].shape[1]\n",
    "        padded_labels = []\n",
    "        for l in labels:\n",
    "            if len(l) < max_len:\n",
    "                l = l + [self.label_pad_token_id] * (max_len - len(l))\n",
    "            else:\n",
    "                l = l[:max_len]\n",
    "            padded_labels.append(l)\n",
    "        batch[\"labels\"] = torch.tensor(padded_labels, dtype=torch.long)\n",
    "        return batch\n",
    "\n",
    "pad_collator = DataCollatorForCausalSFT(\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=8 if torch.cuda.is_available() else None,\n",
    "    label_pad_token_id=-100,\n",
    ")\n",
    "\n",
    "ex = [tokenized[\"train\"][i] for i in range(min(4, len(tokenized[\"train\"])))]\n",
    "b = pad_collator(ex)\n",
    "for k, v in b.items():\n",
    "    print(k, tuple(v.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db89597",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR),\n",
    "    overwrite_output_dir=True,\n",
    "\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=PER_DEVICE_BSZ,\n",
    "    per_device_eval_batch_size=PER_DEVICE_BSZ,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=LOG_STEPS,\n",
    "\n",
    "    fp16=torch.cuda.is_available(),  \n",
    "    fp16_full_eval=False,            \n",
    "    bf16=False,\n",
    "\n",
    "    gradient_checkpointing=True,     \n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    group_by_length=True,\n",
    "\n",
    "    save_safetensors=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec6255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3459' max='3459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3459/3459 07:39, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.447100</td>\n",
       "      <td>1.093547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.443700</td>\n",
       "      <td>0.665516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.464900</td>\n",
       "      <td>0.441094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.223900</td>\n",
       "      <td>0.404773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.210500</td>\n",
       "      <td>0.377015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.096200</td>\n",
       "      <td>0.357304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.129400</td>\n",
       "      <td>0.347935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.210400</td>\n",
       "      <td>0.339929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.043700</td>\n",
       "      <td>0.344236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.070700</td>\n",
       "      <td>0.349208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.371178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.168500</td>\n",
       "      <td>0.318221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.085900</td>\n",
       "      <td>0.347818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.147100</td>\n",
       "      <td>0.317362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.119300</td>\n",
       "      <td>0.315292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.135500</td>\n",
       "      <td>0.316195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.071700</td>\n",
       "      <td>0.305520</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3459, training_loss=0.36564804210151747, metrics={'train_runtime': 460.111, 'train_samples_per_second': 30.051, 'train_steps_per_second': 7.518, 'total_flos': 224882598739968.0, 'train_loss': 0.36564804210151747, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=pad_collator,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    ")\n",
    "\n",
    "train_output = trainer.train()\n",
    "train_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d2236d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='128' max='128' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [128/128 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29438525438308716, 'eval_runtime': 1.9639, 'eval_samples_per_second': 129.841, 'eval_steps_per_second': 65.175, 'epoch': 3.0, 'perplexity': 1.3423009313842171}\n"
     ]
    }
   ],
   "source": [
    "eval_metrics = trainer.evaluate()\n",
    "eval_loss = eval_metrics.get(\"eval_loss\")\n",
    "perplexity = math.exp(eval_loss) if (eval_loss is not None and eval_loss < 20) else float(\"inf\")\n",
    "print({**eval_metrics, \"perplexity\": perplexity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34f32403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to D:\\Game\\Backend\\saved_models\\distilgpt2-npc\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "trainer.save_model(str(OUTPUT_DIR))\n",
    "tokenizer.save_pretrained(str(OUTPUT_DIR))\n",
    "print(f\"Saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50d2e39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4894a4edc4444bb8b649f75cae48d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf4dfa8103643d49d7a5fba64ba929d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46ddff7c753c46d486f5b7e45d786664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635a7f14fab74d1d872a4b7a6eddd173",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu  = evaluate.load(\"bleu\")\n",
    "\n",
    "def distinct_n(corpus: List[str], n: int = 2) -> float:\n",
    "    total = 0\n",
    "    uniq = set()\n",
    "    for text in corpus:\n",
    "        toks = text.strip().split()\n",
    "        if len(toks) < n:\n",
    "            continue\n",
    "        grams = list(zip(*[toks[i:] for i in range(n)]))\n",
    "        total += len(grams)\n",
    "        uniq.update(grams)\n",
    "    return (len(uniq) / total) if total else 0.0\n",
    "\n",
    "def simple_repetition_rate(text: str, window: int = 5) -> float:\n",
    "    toks = text.split()\n",
    "    if not toks:\n",
    "        return 0.0\n",
    "    reps, total = 0, 0\n",
    "    for n in range(1, window + 1):\n",
    "        if len(toks) < n:\n",
    "            continue\n",
    "        grams = list(zip(*[toks[i:] for i in range(n)]))\n",
    "        total += len(grams)\n",
    "        seen = set()\n",
    "        for g in grams:\n",
    "            reps += int(g in seen)\n",
    "            seen.add(g)\n",
    "    return reps / total if total else 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62f4795",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def generate_from_prompts(prompts: List[str]) -> List[str]:\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    for i in range(0, len(prompts), 8): \n",
    "        batch = prompts[i:i+8]\n",
    "        enc = tokenizer(batch, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        gen = model.generate(\n",
    "            **enc,\n",
    "            max_new_tokens=MAX_NEW_TOKENS,\n",
    "            do_sample=True,\n",
    "            top_p=GENERATION_TOP_P,\n",
    "            top_k=GENERATION_TOP_K,\n",
    "            temperature=GENERATION_TEMP,\n",
    "            no_repeat_ngram_size=NO_REPEAT_NGRAM,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "        for j, seq in enumerate(gen):\n",
    "            full = tokenizer.decode(seq, skip_special_tokens=False)\n",
    "            prompt_txt = tokenizer.decode(enc[\"input_ids\"][j], skip_special_tokens=False)\n",
    "            tail = full[len(prompt_txt):] if full.startswith(prompt_txt) else full\n",
    "            tail = tail.split(\"\\n<END>\")[0].strip()\n",
    "            outs.append(tail)\n",
    "    return outs\n",
    "\n",
    "def strip_tags(s: str) -> str:\n",
    "    s = s.replace(\"<END>\", \"\")\n",
    "    s = re.sub(r\"</?[^>]+>\", \"\", s)\n",
    "    return s.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "927704f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL generation metrics: {'rouge1': np.float64(0.18992833196080205), 'rouge2': np.float64(0.12974840563654816), 'rougeL': np.float64(0.17688138681439566), 'bleu': 0.08514192771301035, 'distinct1': 0.09169741697416973, 'distinct2': 0.22683042040623524, 'avg_gen_len': 42.509803921568626, 'repetition_rate': 0.04115911108250407, 'samples_eval': 255}\n",
      "TEST generation metrics: {'rouge1': np.float64(0.195643084823659), 'rouge2': np.float64(0.14085847828871867), 'rougeL': np.float64(0.18454455561326194), 'bleu': 0.09263656757440378, 'distinct1': 0.08961321953876884, 'distinct2': 0.22213960405242122, 'avg_gen_len': 43.1921568627451, 'repetition_rate': 0.03966016412193102, 'samples_eval': 255}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_jsonl(path: Path, max_samples: int = 1000) -> Dict[str, float]:\n",
    "    if not path.exists():\n",
    "        print(f\"[skip] {path} not found\")\n",
    "        return {}\n",
    "    with path.open(\"r\", encoding=\"utf-8\") as f:\n",
    "        items = [json.loads(line) for line in f]\n",
    "    if not items:\n",
    "        print(f\"[skip] {path} empty\")\n",
    "        return {}\n",
    "\n",
    "    if len(items) > max_samples:\n",
    "        random.seed(SEED)\n",
    "        items = random.sample(items, k=max_samples)\n",
    "\n",
    "    prompts = [it[\"prompt\"] for it in items]\n",
    "    refs    = [it[\"target\"].lstrip() for it in items]\n",
    "\n",
    "    preds = generate_from_prompts(prompts)\n",
    "    preds_clean = [strip_tags(p) for p in preds]\n",
    "    refs_clean  = [strip_tags(r) for r in refs]\n",
    "\n",
    "    rouge_res = rouge.compute(predictions=preds_clean, references=refs_clean)\n",
    "    bleu_res  = bleu.compute(predictions=preds_clean, references=[[r] for r in refs_clean])\n",
    "\n",
    "    d1 = distinct_n(preds_clean, 1)\n",
    "    d2 = distinct_n(preds_clean, 2)\n",
    "    avg_len = float(np.mean([len(p.split()) for p in preds_clean])) if preds_clean else 0.0\n",
    "    rep_rate = float(np.mean([simple_repetition_rate(p) for p in preds_clean])) if preds_clean else 0.0\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": rouge_res.get(\"rouge1\", 0.0),\n",
    "        \"rouge2\": rouge_res.get(\"rouge2\", 0.0),\n",
    "        \"rougeL\": rouge_res.get(\"rougeL\", 0.0),\n",
    "        \"bleu\": bleu_res.get(\"bleu\", 0.0),\n",
    "        \"distinct1\": d1,\n",
    "        \"distinct2\": d2,\n",
    "        \"avg_gen_len\": avg_len,\n",
    "        \"repetition_rate\": rep_rate,\n",
    "        \"samples_eval\": len(preds_clean),\n",
    "    }\n",
    "\n",
    "val_gen_metrics  = evaluate_jsonl(VAL_JSONL,  max_samples=1000)\n",
    "test_gen_metrics = evaluate_jsonl(TEST_JSONL, max_samples=1000)\n",
    "print(\"VAL generation metrics:\", val_gen_metrics)\n",
    "print(\"TEST generation metrics:\", test_gen_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c738ded8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: D:\\Game\\Backend\\saved_models\\distilgpt2-npc\\eval_report.json\n",
      "<CONTEXT> Player apologized to Father Jacob at the church door and was heard by visitors.\n",
      "<PLAYER> Jacky, think the rumor will settle now?\n",
      "<NPC>(Jacky) \n",
      "---\n",
      "ive heard about the good news. Keep it steady and we’ll be fine. Keep choosing patience; it bears fruit. Keep coming back. Stay clear of the hedge and keep your word. Stay calm. Stay true to your word and keep choosing patience. Stay healthy. Stay strong. Keep believing that every step\n"
     ]
    }
   ],
   "source": [
    "report = {\n",
    "    \"eval_loss\": float(eval_loss) if eval_loss is not None else None,\n",
    "    \"perplexity\": float(perplexity) if perplexity not in (None, float(\"inf\")) else None,\n",
    "    \"val_generation_metrics\": val_gen_metrics,\n",
    "    \"test_generation_metrics\": test_gen_metrics,\n",
    "    \"config\": {\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"block_size\": BLOCK_SIZE,\n",
    "        \"per_device_bsz\": PER_DEVICE_BSZ,\n",
    "        \"grad_accum\": GRAD_ACCUM,\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"weight_decay\": WEIGHT_DECAY,\n",
    "        \"warmup_steps\": WARMUP_STEPS,\n",
    "        \"generation\": {\n",
    "            \"max_new_tokens\": MAX_NEW_TOKENS,\n",
    "            \"top_p\": GENERATION_TOP_P,\n",
    "            \"top_k\": GENERATION_TOP_K,\n",
    "            \"temperature\": GENERATION_TEMP,\n",
    "            \"no_repeat_ngram\": NO_REPEAT_NGRAM,\n",
    "        },\n",
    "        \"special_tokens\": SPECIAL_TOKENS[\"additional_special_tokens\"],\n",
    "    }\n",
    "}\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUTPUT_DIR / \"eval_report.json\").write_text(json.dumps(report, indent=2), encoding=\"utf-8\")\n",
    "print(\"Wrote:\", OUTPUT_DIR / \"eval_report.json\")\n",
    "\n",
    "def build_npc_prompt(context: str, player_input: str, npc_name: str) -> str:\n",
    "    ctx = context.strip() if context else \"No major events. Player is on good terms with everyone.\"\n",
    "    player = player_input.strip()\n",
    "    name = npc_name.strip()\n",
    "    return f\"<CONTEXT> {ctx}\\n<PLAYER> {player}\\n<NPC>({name}) \"\n",
    "\n",
    "sample_prompt = build_npc_prompt(\n",
    "    \"Player apologized to Father Jacob at the church door and was heard by visitors.\",\n",
    "    \"Jacky, think the rumor will settle now?\",\n",
    "    \"Jacky\"\n",
    ")\n",
    "print(sample_prompt)\n",
    "print(\"---\")\n",
    "print(generate_from_prompts([sample_prompt])[0])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
