{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff3310df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db13e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report: D:\\Game\\Backend\\saved_models\\distilgpt2-npc\\eval_report.json\n",
      "Trainer state (optional): D:\\Game\\Backend\\saved_models\\distilgpt2-npc\\trainer_state.json\n",
      "Out: D:\\Game\\Backend\\saved_models\\distilgpt2-npc\\viz\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ROOT = Path(\"..\").resolve()\n",
    "MODEL_DIR    = PROJECT_ROOT / \"saved_models\" / \"distilgpt2-npc\"\n",
    "\n",
    "EVAL_REPORT_PATH      = MODEL_DIR / \"eval_report.json\"\n",
    "TRAINER_STATE_PATH    = MODEL_DIR / \"trainer_state.json\"   # optional\n",
    "OUTPUT_DIR            = MODEL_DIR / \"viz\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Report:\", EVAL_REPORT_PATH)\n",
    "print(\"Trainer state (optional):\", TRAINER_STATE_PATH)\n",
    "print(\"Out:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41ecdbb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] D:\\Game\\Backend\\saved_models\\distilgpt2-npc\\trainer_state.json not found.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['eval_loss', 'perplexity', 'val_generation_metrics', 'test_generation_metrics', 'config'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_json(path: Path):\n",
    "    if not path.exists():\n",
    "        print(f\"[warn] {path} not found.\")\n",
    "        return None\n",
    "    return json.loads(path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "eval_report   = load_json(EVAL_REPORT_PATH)\n",
    "trainer_state = load_json(TRAINER_STATE_PATH)\n",
    "\n",
    "assert eval_report is not None, \"eval_report.json not found—run training/evaluation first.\"\n",
    "eval_report.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a1f5c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(    metric       val      test\n",
       " 0  ROUGE-1  0.189928  0.195643\n",
       " 1  ROUGE-2  0.129748  0.140858\n",
       " 2  ROUGE-L  0.176881  0.184545\n",
       " 3     BLEU  0.085142  0.092637,\n",
       "             metric       val      test\n",
       " 0       Distinct-1  0.091697  0.089613\n",
       " 1       Distinct-2  0.226830  0.222140\n",
       " 2  Repetition rate  0.041159  0.039660,\n",
       "         split  avg_gen_len\n",
       " 0  Validation    42.509804\n",
       " 1        Test    43.192157,\n",
       "         split   ROUGE-1   ROUGE-2   ROUGE-L      BLEU  Distinct-1  Distinct-2  \\\n",
       " 0  Validation  0.189928  0.129748  0.176881  0.085142    0.091697     0.22683   \n",
       " 1        Test  0.195643  0.140858  0.184545  0.092637    0.089613     0.22214   \n",
       " \n",
       "    Repetition rate  Avg gen len  Samples  Perplexity (val)  Eval loss (val)  \n",
       " 0         0.041159    42.509804      255          1.342301         0.294385  \n",
       " 1         0.039660    43.192157      255               NaN              NaN  )"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = eval_report[\"val_generation_metrics\"]\n",
    "test = eval_report[\"test_generation_metrics\"]\n",
    "\n",
    "df_core = pd.DataFrame({\n",
    "    \"metric\": [\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"BLEU\"],\n",
    "    \"val\":  [val[\"rouge1\"], val[\"rouge2\"], val[\"rougeL\"], val[\"bleu\"]],\n",
    "    \"test\": [test[\"rouge1\"], test[\"rouge2\"], test[\"rougeL\"], test[\"bleu\"]],\n",
    "})\n",
    "\n",
    "df_div = pd.DataFrame({\n",
    "    \"metric\": [\"Distinct-1\", \"Distinct-2\", \"Repetition rate\"],\n",
    "    \"val\":  [val[\"distinct1\"], val[\"distinct2\"], val[\"repetition_rate\"]],\n",
    "    \"test\": [test[\"distinct1\"], test[\"distinct2\"], test[\"repetition_rate\"]],\n",
    "})\n",
    "\n",
    "df_len = pd.DataFrame({\n",
    "    \"split\": [\"Validation\", \"Test\"],\n",
    "    \"avg_gen_len\": [val[\"avg_gen_len\"], test[\"avg_gen_len\"]],\n",
    "})\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    \"split\": [\"Validation\", \"Test\"],\n",
    "    \"ROUGE-1\": [val[\"rouge1\"], test[\"rouge1\"]],\n",
    "    \"ROUGE-2\": [val[\"rouge2\"], test[\"rouge2\"]],\n",
    "    \"ROUGE-L\": [val[\"rougeL\"], test[\"rougeL\"]],\n",
    "    \"BLEU\": [val[\"bleu\"], test[\"bleu\"]],\n",
    "    \"Distinct-1\": [val[\"distinct1\"], test[\"distinct1\"]],\n",
    "    \"Distinct-2\": [val[\"distinct2\"], test[\"distinct2\"]],\n",
    "    \"Repetition rate\": [val[\"repetition_rate\"], test[\"repetition_rate\"]],\n",
    "    \"Avg gen len\": [val[\"avg_gen_len\"], test[\"avg_gen_len\"]],\n",
    "    \"Samples\": [val[\"samples_eval\"], test[\"samples_eval\"]],\n",
    "    \"Perplexity (val)\": [eval_report.get(\"perplexity\", None), None],\n",
    "    \"Eval loss (val)\": [eval_report.get(\"eval_loss\", None), None],\n",
    "})\n",
    "\n",
    "df_core, df_div, df_len.head(), summary_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54732827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Game\\Backend\\saved_models\\distilgpt2-npc\\viz\\perplexity.png\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.bar([\"Validation\"], [eval_report.get(\"perplexity\", None)])\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.title(\"Perplexity\")\n",
    "out_path = OUTPUT_DIR / \"perplexity.png\"\n",
    "plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74df523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[skip] trainer_state.json missing; no loss curve.\n"
     ]
    }
   ],
   "source": [
    "def plot_train_val_loss(trainer_state_json, out_dir: Path):\n",
    "    if not trainer_state_json:\n",
    "        print(\"[skip] trainer_state.json missing; no loss curve.\")\n",
    "        return None\n",
    "\n",
    "    logs = trainer_state_json.get(\"log_history\", [])\n",
    "    if not logs:\n",
    "        print(\"[skip] trainer_state.log_history empty; no loss curve.\")\n",
    "        return None\n",
    "\n",
    "    steps_tr, loss_tr = [], []\n",
    "    steps_ev, loss_ev = [], []\n",
    "\n",
    "    for rec in logs:\n",
    "        if \"loss\" in rec and \"step\" in rec:\n",
    "            steps_tr.append(rec[\"step\"])\n",
    "            loss_tr.append(rec[\"loss\"])\n",
    "        if \"eval_loss\" in rec and \"step\" in rec:\n",
    "            steps_ev.append(rec[\"step\"])\n",
    "            loss_ev.append(rec[\"eval_loss\"])\n",
    "\n",
    "    if not steps_tr and not steps_ev:\n",
    "        print(\"[skip] no (eval_)loss entries in log_history.\")\n",
    "        return None\n",
    "\n",
    "    plt.figure()\n",
    "    if steps_tr:\n",
    "        plt.plot(steps_tr, loss_tr, label=\"Training loss\")\n",
    "    if steps_ev:\n",
    "        plt.plot(steps_ev, loss_ev, label=\"Validation loss\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training vs Validation Loss\")\n",
    "    plt.legend()\n",
    "    out = out_dir / \"loss_training_vs_validation.png\"\n",
    "    plt.savefig(out, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(\"Saved:\", out)\n",
    "    return out\n",
    "\n",
    "_ = plot_train_val_loss(trainer_state, OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b56abc43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Game\\Backend\\saved_models\\distilgpt2-npc\\viz\\quality_rouge_bleu.png\n"
     ]
    }
   ],
   "source": [
    "x = range(len(df_core))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([i - width/2 for i in x], df_core[\"val\"],  width, label=\"Validation\")\n",
    "plt.bar([i + width/2 for i in x], df_core[\"test\"], width, label=\"Test\")\n",
    "plt.xticks(list(x), df_core[\"metric\"])\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Generation Quality — ROUGE/BLEU (Val vs Test)\")\n",
    "plt.legend()\n",
    "out_path = OUTPUT_DIR / \"quality_rouge_bleu.png\"\n",
    "plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "183f2339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Game\\Backend\\saved_models\\distilgpt2-npc\\viz\\diversity_repetition.png\n"
     ]
    }
   ],
   "source": [
    "x = range(len(df_div))\n",
    "width = 0.35\n",
    "\n",
    "plt.figure()\n",
    "plt.bar([i - width/2 for i in x], df_div[\"val\"],  width, label=\"Validation\")\n",
    "plt.bar([i + width/2 for i in x], df_div[\"test\"], width, label=\"Test\")\n",
    "plt.xticks(list(x), df_div[\"metric\"])\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Diversity & Repetition (Val vs Test)\")\n",
    "plt.legend()\n",
    "out_path = OUTPUT_DIR / \"diversity_repetition.png\"\n",
    "plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb61cce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: D:\\Game\\Backend\\saved_models\\distilgpt2-npc\\viz\\avg_generated_length.png\n"
     ]
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.bar(df_len[\"split\"], df_len[\"avg_gen_len\"])\n",
    "plt.ylabel(\"Average generated length (words)\")\n",
    "plt.title(\"Average Generated Length\")\n",
    "out_path = OUTPUT_DIR / \"avg_generated_length.png\"\n",
    "plt.savefig(out_path, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37863ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved summary CSV: D:\\Game\\Backend\\saved_models\\distilgpt2-npc\\viz\\metrics_summary.csv\n",
      "Saved report copy: D:\\Game\\Backend\\saved_models\\distilgpt2-npc\\viz\\eval_report.copy.json\n"
     ]
    }
   ],
   "source": [
    "csv_path = OUTPUT_DIR / \"metrics_summary.csv\"\n",
    "summary_df.to_csv(csv_path, index=False)\n",
    "copy_path = OUTPUT_DIR / \"eval_report.copy.json\"\n",
    "copy_path.write_text(json.dumps(eval_report, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved summary CSV:\", csv_path)\n",
    "print(\"Saved report copy:\", copy_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
