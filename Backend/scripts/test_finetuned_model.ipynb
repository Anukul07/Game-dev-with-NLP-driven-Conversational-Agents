{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c335092a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "model_dir = os.path.join('..', 'saved_models', 'distilgpt2-finetuned-v2')\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a86d2a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Player: Good morning, Jacky.\n",
      "NPC:\n",
      "Jacky is almost always stationed at the guard house on the northwest side of the village. Father Jacob is most often seen here. Father Jacob is most often found in the south. Father Jacob is most often found in the northwest. Father Jacob is usually found in the northwest. Father Jacob is usually found\n",
      "----------------------------------------\n",
      " Player: Where can I find Sarah?\n",
      "NPC:\n",
      "She usually makes way for the children’s room at her house. She always has fresh vegetables at her house. Father Jacob keeps the church keys. Sarah always has her old coins in the northeast corner. Father Jacob keeps the church keys. Where is her old coins? Where can I find her\n",
      "----------------------------------------\n",
      " Player: Mr Dawson, why are your prices so high?\n",
      "NPC:\n",
      "I ask for honest advice. I’m always open to feedback, kind or not. I’m always open to feedback. I’m always open to feedback. If you don’t like it, shop elsewhere. I appreciate your honesty. If you don’t\n",
      "----------------------------------------\n",
      " Player: Tom, do you need help in the workshop?\n",
      "NPC:\n",
      "I just ask for help. The more, the merrier. I get. I should, the merrier. I should. Thanks for letting Acool help. It keeps me busy. Old fashioned.Tom.Tom.I always knows where I are. Old fashioned.Tom.I always look\n",
      "----------------------------------------\n",
      " Player: Sarah, how was the harvest?\n",
      "NPC:\n",
      "Harvest? Busy, but satisfying. Glad the harvest is easier. I should. Thanks for letting Acool handle this. Thanks for letting me know. Thanks for letting me know. Thanks for letting me know. Thanks for letting me know. Thanks for letting me know. Thanks for letting me know\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def generate_npc_reply(prompt, max_new_tokens=60, temperature=0.8):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "  \n",
    "    result = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    reply = result[len(prompt):].strip()\n",
    "    return reply\n",
    "\n",
    "\n",
    "\n",
    "test_prompts = [\n",
    "    \"Player: Good morning, Jacky.\\nNPC:\",\n",
    "    \"Player: Where can I find Sarah?\\nNPC:\",\n",
    "    \"Player: Mr Dawson, why are your prices so high?\\nNPC:\",\n",
    "    \"Player: Tom, do you need help in the workshop?\\nNPC:\",\n",
    "    \"Player: Sarah, how was the harvest?\\nNPC:\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\" {prompt}\")\n",
    "    print(generate_npc_reply(prompt))\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
